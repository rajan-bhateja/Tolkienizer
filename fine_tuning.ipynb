{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-06T14:01:54.521834Z",
     "start_time": "2025-08-06T14:01:50.041656Z"
    }
   },
   "source": "!pip install -q transformers datasets peft trl bitsandbytes accelerate scipy einops",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:02:00.710087Z",
     "start_time": "2025-08-06T14:01:54.764037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=r\"chatml_format_quotes.jsonl\", split=\"train\")"
   ],
   "id": "ff5ee6eea92baf84",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:02:00.732245Z",
     "start_time": "2025-08-06T14:02:00.723515Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "359cbe252fb789fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2994\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:15:08.098745Z",
     "start_time": "2025-08-06T14:08:29.880196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = \"microsoft/phi-2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ],
   "id": "d4ede556c55eaab1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a35a5d69eabe424fb7730601330bbf74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     16\u001B[39m tokenizer = AutoTokenizer.from_pretrained(model, use_fast=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     17\u001B[39m tokenizer.pad_token = tokenizer.eos_token\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbnb_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mauto\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    598\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m model_class.config_class == config.sub_configs.get(\u001B[33m\"\u001B[39m\u001B[33mtext_config\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    599\u001B[39m         config = config.get_text_config()\n\u001B[32m--> \u001B[39m\u001B[32m600\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    601\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    602\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    604\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.\u001B[34m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    605\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(c.\u001B[34m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m._model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    606\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:316\u001B[39m, in \u001B[36mrestore_default_torch_dtype.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    314\u001B[39m old_dtype = torch.get_default_dtype()\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    318\u001B[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5061\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   5051\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   5052\u001B[39m         torch.set_default_dtype(dtype_orig)\n\u001B[32m   5054\u001B[39m     (\n\u001B[32m   5055\u001B[39m         model,\n\u001B[32m   5056\u001B[39m         missing_keys,\n\u001B[32m   5057\u001B[39m         unexpected_keys,\n\u001B[32m   5058\u001B[39m         mismatched_keys,\n\u001B[32m   5059\u001B[39m         offload_index,\n\u001B[32m   5060\u001B[39m         error_msgs,\n\u001B[32m-> \u001B[39m\u001B[32m5061\u001B[39m     ) = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5062\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5063\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5064\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5065\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5066\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5067\u001B[39m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5068\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5069\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5070\u001B[39m \u001B[43m        \u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5071\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5072\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5073\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5074\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5075\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5076\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5077\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5078\u001B[39m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[32m   5079\u001B[39m model.tie_weights()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5524\u001B[39m, in \u001B[36mPreTrainedModel._load_pretrained_model\u001B[39m\u001B[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001B[39m\n\u001B[32m   5521\u001B[39m         args_list = logging.tqdm(args_list, desc=\u001B[33m\"\u001B[39m\u001B[33mLoading checkpoint shards\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   5523\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m args_list:\n\u001B[32m-> \u001B[39m\u001B[32m5524\u001B[39m         _error_msgs, disk_offload_index, cpu_offload_index = \u001B[43mload_shard_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5525\u001B[39m         error_msgs += _error_msgs\n\u001B[32m   5527\u001B[39m \u001B[38;5;66;03m# Adjust offloaded weights name and save if needed\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:974\u001B[39m, in \u001B[36mload_shard_file\u001B[39m\u001B[34m(args)\u001B[39m\n\u001B[32m    972\u001B[39m \u001B[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001B[39;00m\n\u001B[32m    973\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_fsdp_enabled() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_local_dist_rank_0() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_quantized):\n\u001B[32m--> \u001B[39m\u001B[32m974\u001B[39m     disk_offload_index, cpu_offload_index = \u001B[43m_load_state_dict_into_meta_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    975\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_to_load\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    976\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    977\u001B[39m \u001B[43m        \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    978\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexpected_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    979\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreverse_key_renaming_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    980\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    981\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    982\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisk_offload_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    983\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcpu_offload_folder\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcpu_offload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    984\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcpu_offload_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcpu_offload_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    985\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    986\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_safetensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_offloaded_safetensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    987\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    988\u001B[39m \u001B[43m        \u001B[49m\u001B[43munexpected_keys\u001B[49m\u001B[43m=\u001B[49m\u001B[43munexpected_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    989\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    990\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    992\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:882\u001B[39m, in \u001B[36m_load_state_dict_into_meta_model\u001B[39m\u001B[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001B[39m\n\u001B[32m    879\u001B[39m     _load_parameter_into_model(model, param_name, param.to(param_device))\n\u001B[32m    881\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m882\u001B[39m     \u001B[43mhf_quantizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_quantized_param\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    883\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_device\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43munexpected_keys\u001B[49m\n\u001B[32m    884\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    886\u001B[39m     \u001B[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001B[39;00m\n\u001B[32m    887\u001B[39m     \u001B[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001B[39;00m\n\u001B[32m    888\u001B[39m     \u001B[38;5;66;03m# in comparison to the sharded model across GPUs.\u001B[39;00m\n\u001B[32m    889\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_fsdp_enabled() \u001B[38;5;129;01mor\u001B[39;00m is_deepspeed_zero3_enabled():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:250\u001B[39m, in \u001B[36mBnb4BitHfQuantizer.create_quantized_param\u001B[39m\u001B[34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001B[39m\n\u001B[32m    247\u001B[39m         new_value = new_value.T\n\u001B[32m    249\u001B[39m     kwargs = old_value.\u001B[34m__dict__\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     new_value = \u001B[43mbnb\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mParams4bit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequires_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_device\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    252\u001B[39m module._parameters[tensor_name] = new_value\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:336\u001B[39m, in \u001B[36mParams4bit.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    333\u001B[39m device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)\n\u001B[32m    335\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m device \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m device.type != \u001B[33m\"\u001B[39m\u001B[33mmeta\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.bnb_quantized:\n\u001B[32m--> \u001B[39m\u001B[32m336\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_quantize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    337\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    338\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.quant_state \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:295\u001B[39m, in \u001B[36mParams4bit._quantize\u001B[39m\u001B[34m(self, device)\u001B[39m\n\u001B[32m    293\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_quantize\u001B[39m(\u001B[38;5;28mself\u001B[39m, device):\n\u001B[32m    294\u001B[39m     w = \u001B[38;5;28mself\u001B[39m.data.contiguous().to(device)\n\u001B[32m--> \u001B[39m\u001B[32m295\u001B[39m     w_4bit, quant_state = \u001B[43mbnb\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfunctional\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquantize_4bit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    296\u001B[39m \u001B[43m        \u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    297\u001B[39m \u001B[43m        \u001B[49m\u001B[43mblocksize\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mblocksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    298\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcompress_statistics\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompress_statistics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    299\u001B[39m \u001B[43m        \u001B[49m\u001B[43mquant_type\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mquant_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    300\u001B[39m \u001B[43m        \u001B[49m\u001B[43mquant_storage\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mquant_storage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    301\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    302\u001B[39m     \u001B[38;5;28mself\u001B[39m.data = w_4bit\n\u001B[32m    303\u001B[39m     \u001B[38;5;28mself\u001B[39m.quant_state = quant_state\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\bitsandbytes\\functional.py:1008\u001B[39m, in \u001B[36mquantize_4bit\u001B[39m\u001B[34m(A, absmax, out, blocksize, compress_statistics, quant_type, quant_storage)\u001B[39m\n\u001B[32m    983\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Quantize tensor A in blocks of 4-bit values.\u001B[39;00m\n\u001B[32m    984\u001B[39m \n\u001B[32m    985\u001B[39m \u001B[33;03mQuantizes tensor A by dividing it into blocks which are independently quantized.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1004\u001B[39m \u001B[33;03m    - [`QuantState`]: The state object used to undo the quantization.\u001B[39;00m\n\u001B[32m   1005\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1006\u001B[39m input_shape = A.shape\n\u001B[32m-> \u001B[39m\u001B[32m1008\u001B[39m _out, _absmax = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mops\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbitsandbytes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquantize_4bit\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdefault\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m    \u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1010\u001B[39m \u001B[43m    \u001B[49m\u001B[43mblocksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquant_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1012\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquant_storage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1015\u001B[39m code = get_4bit_type(quant_type, device=A.device)\n\u001B[32m   1017\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m compress_statistics:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\torch\\_ops.py:756\u001B[39m, in \u001B[36mOpOverload.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    755\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, /, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m756\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_op\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\torch\\_compile.py:51\u001B[39m, in \u001B[36m_disable_dynamo.<locals>.inner\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     48\u001B[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001B[32m     49\u001B[39m     fn.__dynamo_disable = disable_fn  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdisable_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838\u001B[39m, in \u001B[36mDisableContext.__call__.<locals>._fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    836\u001B[39m _maybe_set_eval_frame(_callback_from_stance(\u001B[38;5;28mself\u001B[39m.callback))\n\u001B[32m    837\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m838\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    839\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    840\u001B[39m     set_eval_frame(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\torch\\library.py:719\u001B[39m, in \u001B[36m_impl.<locals>.register_.<locals>.func_no_dynamo\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    717\u001B[39m \u001B[38;5;129m@torch\u001B[39m._disable_dynamo\n\u001B[32m    718\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfunc_no_dynamo\u001B[39m(*args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m719\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Tolkienizer\\.venv\\Lib\\site-packages\\bitsandbytes\\backends\\cpu\\ops.py:115\u001B[39m, in \u001B[36m_\u001B[39m\u001B[34m(A, blocksize, quant_type, quant_storage)\u001B[39m\n\u001B[32m    112\u001B[39m scaled = blocks / absmax.unsqueeze(-\u001B[32m1\u001B[39m)\n\u001B[32m    114\u001B[39m \u001B[38;5;66;03m# Quantize with the lookup table\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m115\u001B[39m quantized = torch.argmin(\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mabs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscaled\u001B[49m\u001B[43m.\u001B[49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[43m_NF4_QUANT_TABLE\u001B[49m\u001B[43m)\u001B[49m, dim=-\u001B[32m1\u001B[39m, keepdim=\u001B[38;5;28;01mTrue\u001B[39;00m).to(torch.uint8)\n\u001B[32m    117\u001B[39m \u001B[38;5;66;03m# Pack two quantized values per byte\u001B[39;00m\n\u001B[32m    118\u001B[39m packed = quantized[::\u001B[32m2\u001B[39m] << \u001B[32m4\u001B[39m | quantized[\u001B[32m1\u001B[39m::\u001B[32m2\u001B[39m]\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ],
   "id": "43986fcb6851a059"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tokenizer_model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    max_steps=1000,\n",
    "    logging_steps=25,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ],
   "id": "4069c2c5146eb186"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")"
   ],
   "id": "b275dea9cbba8811"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()",
   "id": "716dfe6d0247885a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer.model.save_pretrained(\"./tokenizer_model/tokenizer_lora\")\n",
    "tokenizer.save_pretrained(\"./tokenizer_model/tokenizer_lora\")"
   ],
   "id": "628450253e235d74"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
